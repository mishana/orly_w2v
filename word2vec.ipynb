{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final Assignment - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "NUM_NS = 15\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing - Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"reviews_data.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print the first few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oct nice trendy hotel location not too bad stayed in this hotel for one night as this is fairly new place some of the taxi drivers did not know where it was and or did not want to drive there once have eventually arrived at the hotel was very pleasantly surprised with the decor of the lobby ground floor area it was very stylish and modern found the reception staff geeting me with aloha bit out of place but guess they are briefed to say that to keep up the coroporate image as have starwood preferred guest member was given small gift upon check in it was only couple of fridge magnets in gift box but nevertheless nice gesture my room was nice and roomy there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by bliss the location is not great it is at the last metro stop and you then need to take taxi but if you are not planning on going to see the historic sites in beijing then you will be ok chose to have some breakfast in the hotel which was really tasty and there was good selection of dishes there are couple of computers to use in the communal area as well as pool table there is also small swimming pool and gym area would definitely stay in this hotel again but only if did not plan to travel to central beijing as it can take long time the location is ok if you plan to do lot of shopping as there is big shopping centre just few minutes away from the hotel and there are plenty of eating options around including restaurants that serve dog meat\n",
      "sep great budget hotel stayed two nights at aloft on the most recent trip to china the hotel was very modern and clean the room was spotless and comfortable king sized bed as far as soft beds go in china the staff was very punctual and went out of the way to help my every need including going to store across the street to purchase china mobile sim card for me the buffet breakfast was okay nothing to write home about the lcd screen had movies on demand for rmb and had good selection of western channels including hbo cnn bbc star world etc the gym was small had selection of basic weights and one cable machine there was however new technogym cardio machines with built in lcd tvs which were very good the location is bit out of the way to the central areas of beijing but it is better suited for my needs as need to be in the haidian district being spg platinum there were no upgrades to better room because aloft has policy of not doing any upgrades the sheraton next door is much nicer hotel in my opinion where am writing this from now with an upgraded room but as far as bang for the buck aloft is great place\n",
      "aug excellent value location not big problem we stayed at the aloft beijing haidian for nights from july nd there are lots of reviews that talk about the location being problem but we knew this ahead of time and found that it really wasn an issue the longest we spent in taxi was about minutes we never paid more than rmb for taxi ride which is about cdn and that was to the forbidden city given there are in our family it was no big deal at all as for the rooms they were clean the beds comfortable the wireless internet connection reliable and it was one of the few hotels we found in beijing that would accomodate adults and children we paid about cdn per night that an amazing price it not meant to be star hotel so you can go in expecting that we found the reception staff generally very helpful and friendly they aren the fastest in the world but it wasn unreasonable at all the hotel manager made an effort to speak with us few times and was extremely helpful and welcoming their breakfast buffet was quite good and reasonably priced there are number of good restaurants in the four points sheraton next door so there were lots of options there is massive mall about block away that has other dining options as well the only issue we ran into was few taxi drivers refusing to take us to the summer palace because guess they felt it wasn far enough the minimum rate is rmb but the staff at the four points sheraton which is on busier road than the side street for aloft were quite helpful in finding taxis for us if they weren already there we would definitely stay there again and recommend it to our friends for its excellent value\n"
     ]
    }
   ],
   "source": [
    "for line in lines[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, it seems like the reviews data is already all lower case and with no punctuation.\n",
    "Let us find the vocabulary size first, and determine which words we want to subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_lengths = []\n",
    "\n",
    "from collections import defaultdict\n",
    "word_frequencies = defaultdict(int)\n",
    "\n",
    "for line in lines:\n",
    "    words = line.split()\n",
    "    seq_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2812098),\n",
       " ('and', 1472767),\n",
       " ('to', 1077721),\n",
       " ('was', 903010),\n",
       " ('in', 748274),\n",
       " ('we', 660041),\n",
       " ('of', 614458),\n",
       " ('hotel', 565672),\n",
       " ('for', 544389),\n",
       " ('is', 528043)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_frequencies = sorted(word_frequencies.items(), reverse=True, key=lambda item: item[1])\n",
    "sorted_word_frequencies[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2812098),\n",
       " ('and', 1472767),\n",
       " ('to', 1077721),\n",
       " ('was', 903010),\n",
       " ('in', 748274),\n",
       " ('we', 660041),\n",
       " ('of', 614458),\n",
       " ('hotel', 565672),\n",
       " ('for', 544389),\n",
       " ('is', 528043)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sorted_word_frequencies = list(filter(lambda t: t[1] > 4, sorted_word_frequencies))\n",
    "filtered_sorted_word_frequencies[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sorted_word_frequencies = list(filter(lambda t: t[0] not in stop_words, filtered_sorted_word_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_by_freq, freq = zip(*filtered_sorted_word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_by_freq)\n",
    "MAX_SEQ_LEN = int(np.mean(seq_lengths) + np.std(seq_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE=38436, MAX_SEQ_LEN=306\n"
     ]
    }
   ],
   "source": [
    "print(f'VOCAB_SIZE={VOCAB_SIZE}, MAX_SEQ_LEN={MAX_SEQ_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we will vectorize the vocabulary using a `tf.keras.layers.TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE + 2,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQ_LEN,\n",
    "    vocabulary=vocab_by_freq)\n",
    "\n",
    "# vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the inverse vocabulary to look it up later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'hotel', 'room', 'great', 'stay', 'good', 'staff', 'would', 'location', 'rooms', 'one', 'nice', 'stayed', 'us', 'clean', 'night', 'quot', 'breakfast', 'service']\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "with open('inverse_vocab', 'wb') as f:\n",
    "    pickle.dump(inverse_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Vectorize the data in text_ds.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m text_vector_ds \u001B[38;5;241m=\u001B[39m \u001B[43mtext_ds\u001B[49m\u001B[38;5;241m.\u001B[39mbatch(\u001B[38;5;241m1024\u001B[39m)\u001B[38;5;241m.\u001B[39mprefetch(AUTOTUNE)\u001B[38;5;241m.\u001B[39mmap(vectorize_layer)\u001B[38;5;241m.\u001B[39munbatch()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Obtain Sequences from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255404\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  122    12  1139     2     9     1     1   143    13     1     1     2\n",
      "     1    11    16     1     1     1   559    53    24     1     1     1\n",
      "   302  1625     1     1   193     1     1     1     1     1     1     1\n",
      "    85     1   933     1     1     1  1284   123     1     1     2     1\n",
      "     1   806   480     1     1   347     1     1    93   846    34    40\n",
      "     1     1     1  1191     1   217    73     1   138     7     1     1\n",
      "     1 12951    72     1     1    24     1   709     1     1 24543     1\n",
      "   125     1     1   455     1     1     1  4517     1     1  1881  1552\n",
      "   451   718     1   226    23  1609   382    43     1     1     1     1\n",
      "   240     1   450 20219     1  1609  1338     1  3167    12  4001     1\n",
      "     3     1    12     1  1415     1     1   265     1   103   371     1\n",
      "     1     3     1     1    20    36   397  1324     1   112   315     1\n",
      "   673     1  3405     1     9     1     1     4     1     1     1     1\n",
      "   174   574   367     1     1     1   134     1   119   302     1     1\n",
      "     1     1     1  1118     1   129     1   107     1  1851  1062     1\n",
      "   803     1     1     1     1   170   580     1     1     1    18     1\n",
      "     1     2     1     1    30  1434     1     1     1     6   605     1\n",
      "  1455     1     1   240     1  1529     1   115     1     1  4238    40\n",
      "     1    28     1    80   404     1     1    22    23  1035    80     1\n",
      "   546    40     8   114     5     1     1     2     1     1     1     1\n",
      "     1     1   772     1   327     1   156   803     1     1     1   119\n",
      "   161    21     1     9     1   170     1     1   772     1     1   163\n",
      "     1   186     1     1     1   106   186   479     1     1    92    65\n",
      "     1     1     2     1     1     1   273     1   868   780    55   377\n",
      "    97     1  1334  1993  2487     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0] => ['oct', 'nice', 'trendy', 'hotel', 'location', '[UNK]', '[UNK]', 'bad', 'stayed', '[UNK]', '[UNK]', 'hotel', '[UNK]', 'one', 'night', '[UNK]', '[UNK]', '[UNK]', 'fairly', 'new', 'place', '[UNK]', '[UNK]', '[UNK]', 'taxi', 'drivers', '[UNK]', '[UNK]', 'know', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'want', '[UNK]', 'drive', '[UNK]', '[UNK]', '[UNK]', 'eventually', 'arrived', '[UNK]', '[UNK]', 'hotel', '[UNK]', '[UNK]', 'pleasantly', 'surprised', '[UNK]', '[UNK]', 'decor', '[UNK]', '[UNK]', 'lobby', 'ground', 'floor', 'area', '[UNK]', '[UNK]', '[UNK]', 'stylish', '[UNK]', 'modern', 'found', '[UNK]', 'reception', 'staff', '[UNK]', '[UNK]', '[UNK]', 'aloha', 'bit', '[UNK]', '[UNK]', 'place', '[UNK]', 'guess', '[UNK]', '[UNK]', 'briefed', '[UNK]', 'say', '[UNK]', '[UNK]', 'keep', '[UNK]', '[UNK]', '[UNK]', 'image', '[UNK]', '[UNK]', 'starwood', 'preferred', 'guest', 'member', '[UNK]', 'given', 'small', 'gift', 'upon', 'check', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'couple', '[UNK]', 'fridge', 'magnets', '[UNK]', 'gift', 'box', '[UNK]', 'nevertheless', 'nice', 'gesture', '[UNK]', 'room', '[UNK]', 'nice', '[UNK]', 'roomy', '[UNK]', '[UNK]', 'tea', '[UNK]', 'coffee', 'facilities', '[UNK]', '[UNK]', 'room', '[UNK]', '[UNK]', 'get', 'two', 'complimentary', 'bottles', '[UNK]', 'water', 'plus', '[UNK]', 'toiletries', '[UNK]', 'bliss', '[UNK]', 'location', '[UNK]', '[UNK]', 'great', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'last', 'metro', 'stop', '[UNK]', '[UNK]', '[UNK]', 'need', '[UNK]', 'take', 'taxi', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'planning', '[UNK]', 'going', '[UNK]', 'see', '[UNK]', 'historic', 'sites', '[UNK]', 'beijing', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ok', 'chose', '[UNK]', '[UNK]', '[UNK]', 'breakfast', '[UNK]', '[UNK]', 'hotel', '[UNK]', '[UNK]', 'really', 'tasty', '[UNK]', '[UNK]', '[UNK]', 'good', 'selection', '[UNK]', 'dishes', '[UNK]', '[UNK]', 'couple', '[UNK]', 'computers', '[UNK]', 'use', '[UNK]', '[UNK]', 'communal', 'area', '[UNK]', 'well', '[UNK]', 'pool', 'table', '[UNK]', '[UNK]', 'also', 'small', 'swimming', 'pool', '[UNK]', 'gym', 'area', 'would', 'definitely', 'stay', '[UNK]', '[UNK]', 'hotel', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'plan', '[UNK]', 'travel', '[UNK]', 'central', 'beijing', '[UNK]', '[UNK]', '[UNK]', 'take', 'long', 'time', '[UNK]', 'location', '[UNK]', 'ok', '[UNK]', '[UNK]', 'plan', '[UNK]', '[UNK]', 'lot', '[UNK]', 'shopping', '[UNK]', '[UNK]', '[UNK]', 'big', 'shopping', 'centre', '[UNK]', '[UNK]', 'minutes', 'away', '[UNK]', '[UNK]', 'hotel', '[UNK]', '[UNK]', '[UNK]', 'plenty', '[UNK]', 'eating', 'options', 'around', 'including', 'restaurants', '[UNK]', 'serve', 'dog', 'meat', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[  155     4   390     2    13    36    56     1 10294     1     1     1\n",
      "  1311    84     1   965     1     2     1     1   217     1    15     1\n",
      "     3     1   764     1    39   286   549    25     1   219     1   845\n",
      "    95    41     1   965     1     7     1     1 11802     1    94     1\n",
      "     1     1    90     1   321     1   101   134   377   129     1   512\n",
      "   185     1    50     1  2115   965  2843 13097   360     1     1     1\n",
      "   233    18     1   662   187     1  1260   271     1     1  1567   474\n",
      "     1  2132     1  3319     1  2121     1     1     6   605     1  1177\n",
      "   875   377  4613  4985  4468   165   489   120     1   546     1    23\n",
      "     1   605     1   420  3163     1    11   534   741     1     1    86\n",
      "    53 22737  4039  1065     1  1388     1  1567  1467     1     1     1\n",
      "     6     1     9     1    72     1     1     1    90     1     1   156\n",
      "   444     1   803     1     1     1    75  2090     1     1   445     1\n",
      "   134     1     1     1     1 24544   629     1  3584  2296     1     1\n",
      "     1  3259     1    75     3     1 10294     1  1907     1     1     1\n",
      "     1  3259     1  1173    54    89     1    49   751     2     1     1\n",
      "  1142     1     1  1895     1     1     1     1     1   412     3     1\n",
      "     1   219     1  2436     1     1  3426 10294     1     4    24     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0] => ['sep', 'great', 'budget', 'hotel', 'stayed', 'two', 'nights', '[UNK]', 'aloft', '[UNK]', '[UNK]', '[UNK]', 'recent', 'trip', '[UNK]', 'china', '[UNK]', 'hotel', '[UNK]', '[UNK]', 'modern', '[UNK]', 'clean', '[UNK]', 'room', '[UNK]', 'spotless', '[UNK]', 'comfortable', 'king', 'sized', 'bed', '[UNK]', 'far', '[UNK]', 'soft', 'beds', 'go', '[UNK]', 'china', '[UNK]', 'staff', '[UNK]', '[UNK]', 'punctual', '[UNK]', 'went', '[UNK]', '[UNK]', '[UNK]', 'way', '[UNK]', 'help', '[UNK]', 'every', 'need', 'including', 'going', '[UNK]', 'store', 'across', '[UNK]', 'street', '[UNK]', 'purchase', 'china', 'mobile', 'sim', 'card', '[UNK]', '[UNK]', '[UNK]', 'buffet', 'breakfast', '[UNK]', 'okay', 'nothing', '[UNK]', 'write', 'home', '[UNK]', '[UNK]', 'lcd', 'screen', '[UNK]', 'movies', '[UNK]', 'demand', '[UNK]', 'rmb', '[UNK]', '[UNK]', 'good', 'selection', '[UNK]', 'western', 'channels', 'including', 'hbo', 'cnn', 'bbc', 'star', 'world', 'etc', '[UNK]', 'gym', '[UNK]', 'small', '[UNK]', 'selection', '[UNK]', 'basic', 'weights', '[UNK]', 'one', 'cable', 'machine', '[UNK]', '[UNK]', 'however', 'new', 'technogym', 'cardio', 'machines', '[UNK]', 'built', '[UNK]', 'lcd', 'tvs', '[UNK]', '[UNK]', '[UNK]', 'good', '[UNK]', 'location', '[UNK]', 'bit', '[UNK]', '[UNK]', '[UNK]', 'way', '[UNK]', '[UNK]', 'central', 'areas', '[UNK]', 'beijing', '[UNK]', '[UNK]', '[UNK]', 'better', 'suited', '[UNK]', '[UNK]', 'needs', '[UNK]', 'need', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'haidian', 'district', '[UNK]', 'spg', 'platinum', '[UNK]', '[UNK]', '[UNK]', 'upgrades', '[UNK]', 'better', 'room', '[UNK]', 'aloft', '[UNK]', 'policy', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'upgrades', '[UNK]', 'sheraton', 'next', 'door', '[UNK]', 'much', 'nicer', 'hotel', '[UNK]', '[UNK]', 'opinion', '[UNK]', '[UNK]', 'writing', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'upgraded', 'room', '[UNK]', '[UNK]', 'far', '[UNK]', 'bang', '[UNK]', '[UNK]', 'buck', 'aloft', '[UNK]', 'great', 'place', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[  137    42    87     9     1   106   142     1    13     1     1 10294\n",
      "   803 24544     1    56     1   767   589     1     1   260     1   110\n",
      "     1  1468     1     1     9     1   142     1     1   682     1  1312\n",
      "     1    21     1    73     1     1    30     1     1   575     1  7560\n",
      "     1   317     1   302     1     1    92     1    82   198     1     1\n",
      "  2121     1   302   532     1     1     1  6089     1     1     1     1\n",
      "     1  2194    62   226     1     1     1     1   244     1     1     1\n",
      "   106   215     1     1     1     1     1    10     1     1    15     1\n",
      "    95    39     1   671   126  1149  2795     1     1     1    11     1\n",
      "     1     1    44     1    73     1   803     1     8  2468  1132     1\n",
      "   521     1   198     1  6089   211    16     1     1   270    46     1\n",
      "     1  1022     1     1   165     2     1     1     1    41     1   798\n",
      "     1     1    73     1   138     7   912     1    45     1    32     1\n",
      "     1     1  7230     1     1   489     1     1     1  5362     1     1\n",
      "     1     2   320    96     1  1952     1   879     1    14     1    79\n",
      "     1     1   202    45     1   810     1    18   233     1    78     6\n",
      "     1   690   488     1     1   485     1     6    97     1     1   296\n",
      "   666  1173    54    89     1     1     1   260     1   780     1     1\n",
      "  1374   698     1   282    65     1     1     1   497   780     1    28\n",
      "     1     1   575     1  1390     1     1     1   302  1625  7684     1\n",
      "   119    14     1     1   931   638     1   709     1   232     1     1\n",
      "   219   117     1  2408   151     1  2121     1     1     7     1     1\n",
      "   296   666  1173     1     1     1  5052   294     1     1   182    50\n",
      "     1 10294     1    78    45     1  1178   892     1    14     1     1\n",
      "     1   556     1     1     8   114     5     1     1     1    67     1\n",
      "     1     1   386     1     1    42] => ['aug', 'excellent', 'value', 'location', '[UNK]', 'big', 'problem', '[UNK]', 'stayed', '[UNK]', '[UNK]', 'aloft', 'beijing', 'haidian', '[UNK]', 'nights', '[UNK]', 'july', 'nd', '[UNK]', '[UNK]', 'lots', '[UNK]', 'reviews', '[UNK]', 'talk', '[UNK]', '[UNK]', 'location', '[UNK]', 'problem', '[UNK]', '[UNK]', 'knew', '[UNK]', 'ahead', '[UNK]', 'time', '[UNK]', 'found', '[UNK]', '[UNK]', 'really', '[UNK]', '[UNK]', 'issue', '[UNK]', 'longest', '[UNK]', 'spent', '[UNK]', 'taxi', '[UNK]', '[UNK]', 'minutes', '[UNK]', 'never', 'paid', '[UNK]', '[UNK]', 'rmb', '[UNK]', 'taxi', 'ride', '[UNK]', '[UNK]', '[UNK]', 'cdn', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'forbidden', 'city', 'given', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'family', '[UNK]', '[UNK]', '[UNK]', 'big', 'deal', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'rooms', '[UNK]', '[UNK]', 'clean', '[UNK]', 'beds', 'comfortable', '[UNK]', 'wireless', 'internet', 'connection', 'reliable', '[UNK]', '[UNK]', '[UNK]', 'one', '[UNK]', '[UNK]', '[UNK]', 'hotels', '[UNK]', 'found', '[UNK]', 'beijing', '[UNK]', 'would', 'accomodate', 'adults', '[UNK]', 'children', '[UNK]', 'paid', '[UNK]', 'cdn', 'per', 'night', '[UNK]', '[UNK]', 'amazing', 'price', '[UNK]', '[UNK]', 'meant', '[UNK]', '[UNK]', 'star', 'hotel', '[UNK]', '[UNK]', '[UNK]', 'go', '[UNK]', 'expecting', '[UNK]', '[UNK]', 'found', '[UNK]', 'reception', 'staff', 'generally', '[UNK]', 'helpful', '[UNK]', 'friendly', '[UNK]', '[UNK]', '[UNK]', 'fastest', '[UNK]', '[UNK]', 'world', '[UNK]', '[UNK]', '[UNK]', 'unreasonable', '[UNK]', '[UNK]', '[UNK]', 'hotel', 'manager', 'made', '[UNK]', 'effort', '[UNK]', 'speak', '[UNK]', 'us', '[UNK]', 'times', '[UNK]', '[UNK]', 'extremely', 'helpful', '[UNK]', 'welcoming', '[UNK]', 'breakfast', 'buffet', '[UNK]', 'quite', 'good', '[UNK]', 'reasonably', 'priced', '[UNK]', '[UNK]', 'number', '[UNK]', 'good', 'restaurants', '[UNK]', '[UNK]', 'four', 'points', 'sheraton', 'next', 'door', '[UNK]', '[UNK]', '[UNK]', 'lots', '[UNK]', 'options', '[UNK]', '[UNK]', 'massive', 'mall', '[UNK]', 'block', 'away', '[UNK]', '[UNK]', '[UNK]', 'dining', 'options', '[UNK]', 'well', '[UNK]', '[UNK]', 'issue', '[UNK]', 'ran', '[UNK]', '[UNK]', '[UNK]', 'taxi', 'drivers', 'refusing', '[UNK]', 'take', 'us', '[UNK]', '[UNK]', 'summer', 'palace', '[UNK]', 'guess', '[UNK]', 'felt', '[UNK]', '[UNK]', 'far', 'enough', '[UNK]', 'minimum', 'rate', '[UNK]', 'rmb', '[UNK]', '[UNK]', 'staff', '[UNK]', '[UNK]', 'four', 'points', 'sheraton', '[UNK]', '[UNK]', '[UNK]', 'busier', 'road', '[UNK]', '[UNK]', 'side', 'street', '[UNK]', 'aloft', '[UNK]', 'quite', 'helpful', '[UNK]', 'finding', 'taxis', '[UNK]', 'us', '[UNK]', '[UNK]', '[UNK]', 'already', '[UNK]', '[UNK]', 'would', 'definitely', 'stay', '[UNK]', '[UNK]', '[UNK]', 'recommend', '[UNK]', '[UNK]', '[UNK]', 'friends', '[UNK]', '[UNK]', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:3]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, sampling_table):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # # Build the sampling table for `vocab_size` tokens.\n",
    "  # sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "\n",
    "      # context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "      context_class = context_word.reshape(1, 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      # label = tf.constant([1] + [0] * num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "    labels += [tf.constant([1] + [0] * num_ns, dtype=\"int64\")] * len(positive_skip_grams)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the data is too big (RAM wise) we will generate training data in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m sampling_table \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39msequence\u001B[38;5;241m.\u001B[39mmake_sampling_table(\u001B[38;5;28mlen\u001B[39m(inverse_vocab))\n\u001B[0;32m      7\u001B[0m num_chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m----> 8\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43msequences\u001B[49m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_chunks\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_chunks):\n\u001B[0;32m     10\u001B[0m     targets_path \u001B[38;5;241m=\u001B[39m train_data_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtargets\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_path = Path('train_data_w5_ns15')\n",
    "if not train_data_path.exists():\n",
    "    train_data_path.mkdir()\n",
    "\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(inverse_vocab))\n",
    "\n",
    "num_chunks = 10\n",
    "step = len(sequences) // num_chunks\n",
    "for i in range(num_chunks):\n",
    "    targets_path = train_data_path / f'targets{i}.npy'\n",
    "    contexts_path = train_data_path / f'contexts{i}.npy'\n",
    "    labels_path = train_data_path / f'labels{i}.npy'\n",
    "\n",
    "    if targets_path.exists() and contexts_path.exists() and labels_path.exists():\n",
    "        continue\n",
    "\n",
    "    print(f'{i=}')\n",
    "\n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences[i * step:(i + 1) * step],\n",
    "        window_size=WINDOW_SIZE,\n",
    "        num_ns=NUM_NS,\n",
    "        vocab_size=len(inverse_vocab),\n",
    "        sampling_table=sampling_table,\n",
    "    )\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)[:,:,0]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print('\\n')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "    print(f\"contexts.shape: {contexts.shape}\")\n",
    "    print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "    np.save(targets_path, targets)\n",
    "    np.save(contexts_path, contexts)\n",
    "    np.save(labels_path, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, concatenate all chunks into single big arrays, and take advantage of knowing the vocabulary size by changing the dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = Path('train_data_w5_ns15')\n",
    "targets_path = train_data_path / 'targets'\n",
    "contexts_path = train_data_path / 'contexts'\n",
    "labels_path = train_data_path / 'labels'\n",
    "\n",
    "num_chunks = 10\n",
    "if not targets_path.exists() or not contexts_path.exists() or not labels_path.exists():\n",
    "    targets_list = [np.load(train_data_path / f'targets{i}.npy') for i in range(num_chunks)]\n",
    "    contexts_list = [np.load(train_data_path / f'contexts{i}.npy') for i in range(num_chunks)]\n",
    "    labels_list = [np.load(train_data_path / f'labels{i}.npy') for i in range(num_chunks)]\n",
    "\n",
    "    targets = np.concatenate(targets_list, axis=0, dtype=np.int16)\n",
    "    contexts = np.concatenate(contexts_list, axis=0, dtype=np.int16)\n",
    "    labels = np.concatenate(labels_list, axis=0, dtype=np.int16)\n",
    "\n",
    "    np.save(targets_path, targets)\n",
    "    np.save(contexts_path, contexts)\n",
    "    np.save(labels_path, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a Word2Vec model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(16,)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "39166039\n"
     ]
    }
   ],
   "source": [
    "train_data_path = Path('train_data_w5_ns15')\n",
    "\n",
    "targets = np.load(train_data_path / 'targets.npy')\n",
    "contexts = np.load(train_data_path / 'contexts.npy')\n",
    "labels = np.load(train_data_path / 'labels.npy')\n",
    "print(targets[0].shape)\n",
    "print(contexts[0].shape)\n",
    "print(labels[0])\n",
    "\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 1024\n",
    "BATCH_SIZE = 4096\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Defining the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open('inverse_vocab', 'rb') as f:\n",
    "    inverse_vocabulary = pickle.load(f)\n",
    "\n",
    "VOCAB_SIZE = len(inverse_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "input_target = layers.Input(shape=())\n",
    "input_context = layers.Input(shape=(NUM_NS + 1,))\n",
    "embedding_target = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=1, name=\"w2v_embedding\")(input_target)\n",
    "embedding_context = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=NUM_NS + 1)(input_context)\n",
    "\n",
    "dotted = layers.Dot(axes=[1, 2])([embedding_target, embedding_context])\n",
    "model = models.Model(inputs=[input_target, input_context], outputs=dotted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compile and run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'Adam/Adam/update_1/mul_1' defined at (most recent call last):\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\misha\\AppData\\Local\\Temp\\ipykernel_15112\\4178320667.py\", line 16, in <module>\n      history = model.fit(dataset, epochs=100, callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 671, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 716, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 694, in apply_grad_to_update_var\n      return self._resource_apply_sparse_duplicate_indices(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 1280, in _resource_apply_sparse_duplicate_indices\n      return self._resource_apply_sparse(summed_grad, handle, unique_indices,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 201, in _resource_apply_sparse\n      m_t = tf.compat.v1.assign(m, m * coefficients['beta_1_t'],\nNode: 'Adam/Adam/update_1/mul_1'\nfailed to allocate memory\n\t [[{{node Adam/Adam/update_1/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_209173]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# restore_best_weights - Whether to restore model weights from\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# the epoch with the best value of the monitored quantity.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m early_stopping_cb \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 16\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtensorboard_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_cb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_cb\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'Adam/Adam/update_1/mul_1' defined at (most recent call last):\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\misha\\AppData\\Local\\Temp\\ipykernel_15112\\4178320667.py\", line 16, in <module>\n      history = model.fit(dataset, epochs=100, callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 671, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 716, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 694, in apply_grad_to_update_var\n      return self._resource_apply_sparse_duplicate_indices(\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 1280, in _resource_apply_sparse_duplicate_indices\n      return self._resource_apply_sparse(summed_grad, handle, unique_indices,\n    File \"C:\\Users\\misha\\miniconda3\\envs\\orly310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 201, in _resource_apply_sparse\n      m_t = tf.compat.v1.assign(m, m * coefficients['beta_1_t'],\nNode: 'Adam/Adam/update_1/mul_1'\nfailed to allocate memory\n\t [[{{node Adam/Adam/update_1/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_209173]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"my_logs\")\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"word2vec_model_w5_ns15_ckpt.h5\", monitor='accuracy', save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='accuracy')\n",
    "\n",
    "history = model.fit(dataset, epochs=100, callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saving the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"word2vec_model_w5_ns15.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Retrieve the learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "print(f'weights shape is {weights.shape}')\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "with open('inverse_vocab', 'rb') as f:\n",
    "    inverse_vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, save the vectors to disk in a `.parquet` file with a word column as key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "df = pd.DataFrame({'word': inverse_vocabulary[2:], 'embedding': list(weights[2:])})\n",
    "df.set_index('word', inplace=True)\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'word2vec_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hotel</th>\n",
       "      <td>[-0.12856515, 0.15770362, 0.06505656, 0.234301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room</th>\n",
       "      <td>[-0.08665641, 0.22643305, 0.17230006, 0.242368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>[0.18022697, -0.051985245, 0.3708178, -0.03778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>[-0.048565857, 0.11214562, -0.022309598, 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>[0.07658313, -0.07445546, 0.08375567, -0.03007...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               embedding\n",
       "word                                                    \n",
       "hotel  [-0.12856515, 0.15770362, 0.06505656, 0.234301...\n",
       "room   [-0.08665641, 0.22643305, 0.17230006, 0.242368...\n",
       "great  [0.18022697, -0.051985245, 0.3708178, -0.03778...\n",
       "stay   [-0.048565857, 0.11214562, -0.022309598, 0.208...\n",
       "good   [0.07658313, -0.07445546, 0.08375567, -0.03007..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze the learned Word Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find Most Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "embeddings_path='word2vec_embeddings.parquet'\n",
    "word_vectors_table = pq.read_table(embeddings_path).to_pandas()\n",
    "\n",
    "def find_most_similar(word: str, k: int = 10, word_vectors: pd.DataFrame = word_vectors_table):\n",
    "    word_vector = word_vectors.loc[word].embedding\n",
    "\n",
    "    weights = np.asarray(list(word_vectors_table.embedding.values))\n",
    "    similarities = cosine_similarity(word_vector.reshape(1, -1), weights).flatten()\n",
    "\n",
    "    most_similar_idxs = np.argsort(similarities)[::-1][1:k + 1]  # skip the word itself\n",
    "    return [(word_vectors.index[idx], similarities[idx]) for idx in most_similar_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('vehicle', 0.6907699),\n ('van', 0.60751176),\n ('cars', 0.6057887),\n ('parked', 0.5898296),\n ('suv', 0.5631365),\n ('valet', 0.53012586),\n ('rental', 0.52415824),\n ('trolley', 0.5229506),\n ('taxi', 0.51901245),\n ('shuttle', 0.51762193)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize with the `Tensorboard Embedding Projector`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the vectors from the `.parquet` file to a 2d numpy array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(38436, 128)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "embeddings_path='word2vec_embeddings.parquet'\n",
    "word_vectors_table = pq.read_table(embeddings_path).to_pandas()\n",
    "\n",
    "weights = np.asarray(list(word_vectors_table.embedding.values))\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save vectors metadata in `.tsv` format for the use of the `Embedding Projector`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path('my_logs')\n",
    "if not logs_path.exists():\n",
    "    logs_path.mkdir()\n",
    "\n",
    "with open(logs_path / 'metadata.tsv', 'w', encoding='utf-8') as f:\n",
    "    for word in word_vectors_table.index:\n",
    "      f.write(word + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Produce a checkpoint for the `Embedding Projector`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'my_logs\\\\embedding.ckpt-1'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weights = tf.Variable(weights)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(logs_path / \"embedding.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configure and run the `Embedding Projector`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(logs_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'my_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find Clusters using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}