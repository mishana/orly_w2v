{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Perceptron - sklearn](#Perceptron)\n",
    "* [Tensorflow & Keras](#tensorflow_keras)\n",
    "    * [Tensors](#tensors)\n",
    "    * [Derivative computation](#tensors_derivatives)\n",
    "* [Neural networks](#neural_networks)\n",
    "    * [Layers in Keras](#keras_layers)\n",
    "    * [Sequential Model - classification MLP](#sequential_api)\n",
    "    * [Sequential Model - Regression MLP](#sequential_regression_mlp)\n",
    "    * [Functional Model - Regression MLP](#functional_regression_mlp)\n",
    "    * [Save & Load a model](#save_and_load)\n",
    "    * [Callbacks & Tensorboard](#callbacks_and_tb)\n",
    "    * [Multi input model](#multi_input_model)\n",
    "    * [Multi input & output model](#multi_input_output_model)\n",
    "    * [Subclassing API Wide & Deep](#subclass_api)\n",
    "    * [Hyperparameters Search](#hyperparam_search)\n",
    "    * [Batch Normalization](#batch_normalization)\n",
    "    * [Gradient Clipping](#grad_clipping)\n",
    "    * [transfer_learning](#transfer_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# inline plot \n",
    "%matplotlib inline  \n",
    "# default figure size \n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make our sets reproducible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Perceptron (sklearn)  <a class=\"anchor\" id=\"Perceptron\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Tensorflow & Keras  <a class=\"anchor\" id=\"tensorflow_keras\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors & Variables  <a class=\"anchor\" id=\"tensors\"></a>\n",
    "A tensor is the data flowing through the network \n",
    "Tensors are multi-dimensional arrays with a uniform type (like float/int/bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant generates a constant tensor which is immutable and cant be changed once defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using numpy array\n",
    "a = tf.constant(np.array([2., 4., 5.]))\n",
    "# using nested lists\n",
    "u = tf.constant([[1],[2],[3]], dtype=tf.float32)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable is a special tensor which is mutable and can be updated during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "# assignment\n",
    "V.assign(2 * V) # => [[2., 4., 6.], [8., 10., 12.]]\n",
    "V[0, 1].assign(42) # => [[2., 42., 6.], [8., 10., 12.]]\n",
    "V[:, 2].assign([0., 1.]) # => [[2., 42., 0.], [8., 10., 1.]]\n",
    "V.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.]) # => [[100., 42., 0.], [8., 10., 200.]]\n",
    "\n",
    "# same as above using dtype for casting the values to float\n",
    "V = tf.Variable([[1, 2, 3],[4, 5, 6]], shape=[2, 3], dtype=tf.float32)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow provides various operations to manipulate tensors using which the neural network is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vu\n",
    "x = tf.matmul(v, u, transpose_b=False)\n",
    "\n",
    "# typing matters: this want work cant add int with float\n",
    "# tf.constant(2.) + tf.constant(40)\n",
    "tf.constant(2.) * tf.cast(tf.constant(40), tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative computation <a class=\"anchor\" id=\"tensors_derivatives\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-0.63425803, -0.31910765],\n",
       "        [-1.2685161 , -0.6382153 ],\n",
       "        [-1.9027741 , -0.95732296]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.63425803, -0.31910765], dtype=float32)>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make the result reproducible \n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# the weights\n",
    "W = tf.Variable(tf.random.normal((3, 2)), name='W')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "y_true = [[5., 6.]]\n",
    "\n",
    "# tf.GradientTape context will automatically record every operation that involves a variable, and finally we\n",
    "# ask this tape to compute the gradients of the result loss with regards to both variables W and b\n",
    "\n",
    "# The tape is automatically erased immediately after you call its gradient() method, so\n",
    "# you will get an exception if you try to call gradient() twice, using persistent=True overrides this \n",
    "# and allows us to call gradient() multiple times\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x @ W + b # the same as  y = tf.matmul(x, W) + b\n",
    "    y = tf.sigmoid(z) # no really need for a sigmoid here, just for demonstration \n",
    "    loss = tf.reduce_mean((y_true - y)**2)\n",
    "\n",
    "[dloss_dW, dloss_db] = tape.gradient(loss, [W, b])\n",
    "[dloss_dW, dloss_db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks  <a class=\"anchor\" id=\"neural_networks\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \n",
    "    \"Trouser\", \n",
    "    \"Pullover\", \n",
    "    \"Dress\", \n",
    "    \"Coat\",\n",
    "    \"Sandal\", \n",
    "    \"Shirt\", \n",
    "    \"Sneaker\", \n",
    "    \"Bag\", \n",
    "    \"Ankle boot\"\n",
    "]\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(f'''\n",
    "number of samplesin train: {X_train_full.shape[0]}\n",
    "number of features: {X_train_full.shape[1]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scale features to be in the unit range [0,1]\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# don't forget scaling test too!!!\n",
    "X_test = X_test / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in Keras  <a class=\"anchor\" id=\"keras_layers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.9643673 , 0.9054539 , 0.3298724 , 0.42120355],\n",
       "       [0.9071679 , 0.729444  , 0.4418784 , 0.56132346]], dtype=float32)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer = keras.layers.Dense(units=4, \n",
    "                                 activation='sigmoid', \n",
    "                                 use_bias=True, \n",
    "                                 kernel_initializer='glorot_normal', \n",
    "                                 name='my_dense_layer')\n",
    "\n",
    "input_batch = tf.constant([[1.,2.,3.],[0.,1.,2.]])\n",
    "\n",
    "dense_layer(input_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_dense_layer/kernel:0' shape=(3, 4) dtype=float32, numpy=\n",
       " array([[ 0.25954103, -0.14177108, -0.6353535 , -0.02958083],\n",
       "        [-0.920836  ,  0.27364743, -0.8382316 ,  0.7370411 ],\n",
       "        [ 0.3356731 ,  0.17979145, -0.19608879, -0.2965222 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'my_dense_layer/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "Transforms a categorical variable to a vector.\n",
    "\n",
    "Equivalent to transforming the index to a one-hot vector and then multiplying by an embedding matrix \n",
    "\n",
    "[The, dog, eats] -> [2, 30, 15] -> Embedding layer -> vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[-0.0129756 ,  0.03094221,  0.04268317, -0.0170565 ],\n",
       "        [ 0.03388727,  0.00856883, -0.00980901,  0.03036347],\n",
       "        [ 0.00992926, -0.02981392,  0.04883833,  0.01197966]],\n",
       "\n",
       "       [[-0.02618294, -0.00915499,  0.04792578, -0.04209584],\n",
       "        [-0.0129756 ,  0.03094221,  0.04268317, -0.0170565 ],\n",
       "        [ 0.03388727,  0.00856883, -0.00980901,  0.03036347]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size is taken to be 100\n",
    "# vector size is 4\n",
    "\n",
    "indexed_values = tf.constant([[1,2,3],[0,1,2]])\n",
    "embedding_layer = keras.layers.Embedding(input_dim=100, output_dim=4, mask_zero=True)\n",
    "embedding_layer(indexed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [False,  True,  True]])>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the vectors to be discarded\n",
    "embedding_layer.compute_mask(indexed_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model - Classification <a class=\"anchor\" id=\"sequential_api\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras. (responsible for managing layer names etc.)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# sequential API\n",
    "model = keras.models.Sequential()\n",
    "# flattens tensor shape (28,28) to (784,)\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", name='my_hidden'))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Equivalent \n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28, 28]),\n",
    "#     keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7fd245a6f2b0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fd246054a90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fd246335240>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fd2465b0978>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fd2461ac940>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fd245e7dc18>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fd242304860>]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the layers of the model\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the first hidden layer\n",
    "hidden1 = model.layers[1]\n",
    "# its name\n",
    "layer_name = hidden1.name\n",
    "# can access a layer by name\n",
    "hidden1 = model.get_layer('my_hidden')\n",
    "\n",
    "# get the layer weights\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "print(f'''\n",
    "hidden layer 1 name: {layer_name}\n",
    "weights shape: {weights.shape}\n",
    "bais shape: {biases.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compile the model - Configures the model for training.\n",
    "# categorical crossentropy in the exact same loss as softmax classification model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# equivalent\n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.SGD(),\n",
    "#               metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show training params like numbber of epochs ran \n",
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).reset_index().rename(columns={'index': 'epoch'})[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test\n",
    "# loss and metric results are returned as a dict, with each key being the name of the metric\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "X_new = X_test[:10]\n",
    "y_proba = model.predict(X_new)\n",
    "\n",
    "print(f'''\n",
    "raw predictions: \n",
    "{y_proba.round(2)}\n",
    "predicted classes \n",
    "{np.array(class_names)[np.argmax(model.predict(X_new), axis=-1)]}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, len(X_new), index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Sequential API - Regression MLP  <a class=\"anchor\" id=\"sequential_regression_mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# information about the data\n",
    "print(housing['DESCR'])\n",
    "\n",
    "print('Data table')\n",
    "print('===========')\n",
    "print(fetch_california_housing(as_frame=True)['data'].head())\n",
    "\n",
    "# target/label = MedHouseVal\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# features scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit only on train\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform validation and test\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Define the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(f'MSE on test {mse_test}')\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API - Regression MLP  <a class=\"anchor\" id=\"functional_regression_mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Functional API\n",
    "# Allows to define more complex networks \n",
    "# The following architecture is called \"wide and deep\" \n",
    "# “Wide & Deep Learning for Recommender Systems,” Heng-Tze Cheng et al. (2016).\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "# this layer just concatenates the input_ tensor and hidden2 tensor along the last axis\n",
    "concat = keras.layers.concatenate([input_, hidden2], axis=-1)\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(f'MSE on test {mse_test}')\n",
    "\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading <a class=\"anchor\" id=\"save_and_load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "model.save(\"my_keras_model.h5\", include_optimizer=True)\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "y_pred = model.predict(X_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only saves the weights the network will need to be defined via code\n",
    "model.save_weights(\"my_keras_weights.ckpt\")\n",
    "\n",
    "# define the model here\n",
    "# ....\n",
    "# ....\n",
    "model.load_weights(\"my_keras_weights.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using callbacks & Tensorboard <a class=\"anchor\" id=\"callbacks_and_tb\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start the TensorBoard server, one option is to open a terminal, if needed activate the virtualenv where you installed TensorBoard, go to this notebook's directory, then type:\n",
    "\n",
    "# $ tensorboard --logdir=./my_logs --port=6006\n",
    "# You can then open your web browser to localhost:6006 and use TensorBoard. Once you are done, press Ctrl-C in the terminal window, this will shutdown the TensorBoard server.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=\"my_keras_model_ckpt.h5\",     \n",
    "                                                monitor='val_loss',\n",
    "                                                save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# custom callback\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, \n",
    "               early_stopping_cb,\n",
    "               PrintValTrainRatioCallback(),\n",
    "               keras.callbacks.TensorBoard(run_logdir)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Multi Input model <a class=\"anchor\" id=\"multi_input_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Splitting the input - for multiple inputs model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "\n",
    "# define the model by specifying the inputs and output tensors \n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Generate the data - split the features \n",
    "\n",
    "# X_train_A - fetaure 0-4 (5)\n",
    "# X_train_B - feature 2-8 (6)\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# fit\n",
    "history = model.fit(x=(X_train_A, X_train_B), \n",
    "                    y=y_train, \n",
    "                    epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid)\n",
    "                   )\n",
    "# evaluate on test\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "# make a prediction\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Multi Input Multi output Model <a class=\"anchor\" id=\"multi_input_output_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "# for each of the outputs tensors a loss is specified and a weight for aggregating the losses \n",
    "model.compile(loss={'main_output': \"mse\", 'aux_output':\"mse\"}, \n",
    "              loss_weights={'main_output': 0.9, 'aux_output':0.1}, \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train_A, X_train_B], \n",
    "                    y=[y_train, y_train], \n",
    "                    epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "# we see that the lower the weight the bigger the loss - as it is less influential on the final loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    x=[X_test_A, X_test_B], \n",
    "    y=[y_test, y_test]\n",
    ")\n",
    "\n",
    "print(f'metrics: {model.metrics_names}')\n",
    "model.metrics_names()\n",
    "print(f\"\"\"\n",
    "main loss {main_loss}\n",
    "aux loss {aux_loss}\n",
    "total loss {total_loss}\n",
    "\"\"\")\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## The subclassing API <a class=\"anchor\" id=\"subclass_api\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# define the model as a derived class of Model class\n",
    "class WideAndDeepModel(keras.models.Model()):\n",
    "    \n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "    \n",
    "        super().__init__(**kwargs)\n",
    "        # all layers are declared in the constructor\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # forward pass \n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")\n",
    "\n",
    "# need to be buit before summary is available\n",
    "model.build(input_shape=[(None, 8), (None, 8)])\n",
    "model.summary()\n",
    "\n",
    "# plot_model does not work out of the box :)\n",
    "# inputs = [keras.layers.Input(shape=(None,8)),keras.layers.Input(shape=(None,8))]\n",
    "# model = keras.models.Model(inputs=inputs,outputs=model.call(inputs))\n",
    "# keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", \n",
    "              loss_weights=[0.9, 0.1], \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3)\n",
    "             )\n",
    "history = model.fit(x=(X_train_A, X_train_B), \n",
    "                    y=(y_train, y_train), \n",
    "                    epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Hyperparameters search using scikit learn API <a class=\"anchor\" id=\"hyperparam_search\"></a>\n",
    "a wrapper Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_fn=build_model)\n",
    "\n",
    "# for classifier \n",
    "# keras_cls = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=10,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "mse_test = keras_reg.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# specify the search domain\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(), # sampling from A loguniform\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X=X_train, y=y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "print(f'''\n",
    "best params: {rnd_search_cv.best_params_}\n",
    "best score: {rnd_search_cv.best_score_}\n",
    "''')\n",
    "\n",
    "# KerasRegressor\n",
    "rnd_search_cv.best_estimator_\n",
    "\n",
    "best_model = model = rnd_search_cv.best_estimator_.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# standardizing per pixel \n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization <a class=\"anchor\" id=\"batch_normalization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(units=300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Using before activations - note that now we define the activation outside \n",
    "# of the layer definition we also don't use bias, since it is learned by the bias\n",
    "# parameter of the BN layer\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28, 28]),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(300, use_bias=False),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Activation(\"relu\"),\n",
    "#     keras.layers.Dense(100, use_bias=False),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Activation(\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clipping  <a class=\"anchor\" id=\"grad_clipping\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning  <a class=\"anchor\" id=\"transfer_learning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "\n",
    "# if we won't clone model A, its weights will be updated with modelB_on_A\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                 \n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# You must always compile your model after you freeze or unfreeze layers.\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2env]",
   "language": "python",
   "name": "conda-env-tf2env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
